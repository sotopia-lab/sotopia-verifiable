# Werewolf Agent Loop Config for Expert Iteration Training
# Standalone config - doesn't depend on verl's internal configs

# Data config
data:
  train_files: ${oc.env:PWD}/data/werewolf_scenarios.parquet
  val_files: ${oc.env:PWD}/data/werewolf_scenarios.parquet
  train_batch_size: 8
  micro_batch_size_per_gpu: 2
  max_length: 4096
  prompt_key: null  # Agent loop generates prompts dynamically
  response_key: null

# Model config
actor_rollout_ref:
  hybrid_engine: true

  model:
    path: Qwen/Qwen2.5-7B-Instruct
    trust_remote_code: true
    external_lib: null
    enable_gradient_checkpointing: true
    fsdp_config:
      model_dtype: bf16

  # Rollout config with our agent loop
  rollout:
    name: vllm
    mode: async
    tensor_model_parallel_size: 1
    prompt_length: 2048
    response_length: 2048
    temperature: 0.7
    top_p: 0.9
    max_game_turns: 40

    # Agent loop settings
    agent:
      num_workers: 4
      default_agent_loop: werewolf_agent
      agent_loop_config_path: null

    # Validation settings
    val_kwargs:
      temperature: 0.3
      top_p: 0.9

# Disable components not needed
reward_model:
  enable: false

# Trainer settings
trainer:
  project_name: werewolf-expert-iter
  experiment_name: baseline
  total_epochs: 1
  nnodes: 1
  n_gpus_per_node: 1
  save_freq: 100
  test_freq: 50
  val_before_train: true
  logger: ['console']
  default_local_dir: checkpoints/werewolf/${trainer.experiment_name}
  device: cuda

# Algorithm settings
algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: gae
  use_kl_in_reward: false
